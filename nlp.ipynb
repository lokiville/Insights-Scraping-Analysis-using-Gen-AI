{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (2.6.0+cpu)\n",
      "Requirement already satisfied: transformers in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (4.51.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: keybert in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (0.9.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keybert) (13.7.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.4.0->keybert) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers sentence-transformers keybert scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Judge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Judge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_and_normalize(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    # Remove special characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    cleaned_words = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in words\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "def process_articles(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "    cleaned_articles = []\n",
    "    for article in articles:\n",
    "        # Merge content and pdf_content if available\n",
    "        content = article.get('content', '')\n",
    "        pdf_content = article.get('pdf_content', '')\n",
    "        merged_text = content + ' ' + pdf_content if pdf_content else content\n",
    "        cleaned_text = clean_and_normalize(merged_text)\n",
    "        # Preserve all original metadata except content/pdf_content, add cleaned_text\n",
    "        cleaned_article = {\n",
    "            k: v for k, v in article.items()\n",
    "            if k not in ['content', 'pdf_content']\n",
    "        }\n",
    "        cleaned_article['cleaned_text'] = cleaned_text\n",
    "        cleaned_articles.append(cleaned_article)\n",
    "    return cleaned_articles\n",
    "\n",
    "# 1. Process and clean articles\n",
    "cleaned_kpmg = process_articles('kpmg_articles.json')\n",
    "cleaned_pwc = process_articles('pwc_articles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry list with keywords\n",
    "INDUSTRY_KEYWORDS = {\n",
    "    \"Artificial Intelligence\": [\"ai\", \"artificial intelligence\", \"machine learning\", \"deep learning\", \"neural network\", \"computer vision\"],\n",
    "    \"Supply Chain\": [\"supply chain\", \"logistics\", \"inventory\", \"procurement\", \"distribution network\"],\n",
    "    \"Energy and Renewables\": [\"energy\", \"renewable\", \"solar\", \"wind power\", \"hydroelectric\", \"carbon footprint\"],\n",
    "    \"Cyber Security\": [\"cyber security\", \"ransomware\", \"phishing\", \"data breach\", \"encryption\", \"firewall\"],\n",
    "    \"Economy and Growth\": [\"gdp\", \"economic growth\", \"inflation\", \"market trends\", \"fiscal policy\", \"monetary policy\"],\n",
    "    \"ESG\": [\"esg\", \"environmental social governance\", \"sustainability\", \"carbon neutral\", \"social responsibility\"],\n",
    "    \"Technology\": [\"technology\", \"digital transformation\", \"cloud computing\", \"iot\", \"blockchain\", \"5g\"],\n",
    "    \"Risk and Regulation\": [\"risk management\", \"compliance\", \"regulation\", \"legal framework\", \"audit\"],\n",
    "    \"Workforce\": [\"workforce\", \"employee engagement\", \"talent acquisition\", \"skills gap\", \"remote work\"],\n",
    "    \"Transformation\": [\"digital transformation\", \"business transformation\", \"change management\", \"operational excellence\"],\n",
    "    \"Global Capability Centres\": [\"gcc\", \"global capability centres\", \"shared services\", \"offshoring\", \"captive centers\"],\n",
    "    \"Assurance\": [\"assurance\", \"quality control\", \"audit\", \"compliance\", \"risk assessment\"],\n",
    "    \"Trade and Tariffs\": [\"trade\", \"tariff\", \"import export\", \"customs\", \"free trade agreement\"],\n",
    "    \"Financial Services\": [\"finance\", \"banking\", \"insurance\", \"investment\", \"asset management\", \"fintech\"],\n",
    "    \"Healthcare\": [\"healthcare\", \"pharmaceutical\", \"medical devices\", \"patient care\", \"telemedicine\"],\n",
    "    \"Manufacturing\": [\"manufacturing\", \"industry 4.0\", \"smart factory\", \"production line\", \"lean manufacturing\"]\n",
    "}\n",
    "\n",
    "def classify_industry(text, threshold=2):\n",
    "    \"\"\"\n",
    "    Classify text into industry based on keyword counts\n",
    "    Returns the industry with highest keyword matches\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    scores = {industry: 0 for industry in INDUSTRY_KEYWORDS}\n",
    "    \n",
    "    for industry, keywords in INDUSTRY_KEYWORDS.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text:\n",
    "                scores[industry] += 1\n",
    "    \n",
    "    # Get industry with maximum score\n",
    "    max_industry = max(scores, key=scores.get)\n",
    "    \n",
    "    # Only return if meets minimum threshold, else 'Other'\n",
    "    return max_industry if scores[max_industry] >= threshold else \"Other\"\n",
    "\n",
    "def add_industry_classification(articles):\n",
    "    for article in articles:\n",
    "        article['industry'] = classify_industry(article['cleaned_text'])\n",
    "    return articles\n",
    "\n",
    "# 2. Add industry classification\n",
    "classified_kpmg = add_industry_classification(cleaned_kpmg)\n",
    "classified_pwc = add_industry_classification(cleaned_pwc)\n",
    "\n",
    "# 3. Save results\n",
    "with open('classified_kpmg.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(classified_kpmg, f, indent=2)\n",
    "with open('classified_pwc.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(classified_pwc, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.68.1)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\judge\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "Downloading google_api_python_client-2.167.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.7/13.2 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.6/13.2 MB 18.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.2 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 17.7 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.29.4-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 3.9/4.3 MB 19.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 17.1 MB/s eta 0:00:00\n",
      "Installing collected packages: uritemplate, rsa, protobuf, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.1\n",
      "    Uninstalling grpcio-1.68.1:\n",
      "      Successfully uninstalled grpcio-1.68.1\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.167.0 google-auth-2.39.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 proto-plus-1.26.1 protobuf-5.29.4 rsa-4.9.1 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\Judge\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.16.2 requires keras>=3.0.0, which is not installed.\n",
      "tensorflow-intel 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please set your GEMINI_API_KEY environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set your GEMINI_API_KEY environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m genai\u001b[38;5;241m.\u001b[39mconfigure(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Gemini 2.5 Flash model\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Please set your GEMINI_API_KEY environment variable."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Gemini API key (set your environment variable GEMINI_API_KEY before running)\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your GEMINI_API_KEY environment variable.\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize Gemini 2.5 Flash model\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash-preview-04-17\")\n",
    "\n",
    "def build_prompt(article):\n",
    "    \"\"\"\n",
    "    Builds the prompt for Gemini 2.5 Flash to extract themes and keywords\n",
    "    following the LLM-TAKE framework.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert analyst using the LLM-TAKE framework.\n",
    "\n",
    "Given the following article text, perform the following:\n",
    "\n",
    "1. Break the text into semantic sections if needed.\n",
    "2. Extract 3–7 main themes or topics as concise phrases (not single words).\n",
    "3. List 5–10 significant keywords (single words or short phrases).\n",
    "4. For each theme, provide a supporting quote or sentence from the article.\n",
    "5. Avoid generic or irrelevant keywords (e.g., \"article\", \"information\", \"read\").\n",
    "6. Remove any sensitive or irrelevant terms.\n",
    "7. Ensure diversity and coverage of the article’s content.\n",
    "\n",
    "Return the result in this JSON format:\n",
    "{{\n",
    "  \"title\": \"{article['title']}\",\n",
    "  \"themes\": [\n",
    "    {{\"theme\": \"...\", \"evidence\": \"...\"}}\n",
    "  ],\n",
    "  \"keywords\": [\"...\", \"...\", \"...\"]\n",
    "}}\n",
    "\n",
    "Article text:\n",
    "{article['cleaned_text']}\n",
    "\"\"\"\n",
    "\n",
    "def extract_themes_keywords(article):\n",
    "    \"\"\"\n",
    "    Calls Gemini 2.5 Flash API with the prompt and parses the JSON response.\n",
    "    Retries once if JSON parsing fails.\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(article)\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"temperature\": 0.3, \"max_output_tokens\": 512}\n",
    "        )\n",
    "        result_text = response.text.strip()\n",
    "        # Attempt to parse JSON from response\n",
    "        return json.loads(result_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Retry once with a simpler prompt or return minimal fallback\n",
    "        print(f\"Warning: JSON parsing failed for article '{article['title']}'. Returning minimal output.\")\n",
    "        return {\n",
    "            \"title\": article.get(\"title\", \"\"),\n",
    "            \"themes\": [],\n",
    "            \"keywords\": []\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Load classified articles from previous step\n",
    "    with open(\"classified_kpmg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        kpmg_articles = json.load(f)\n",
    "    with open(\"classified_pwc.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        pwc_articles = json.load(f)\n",
    "\n",
    "    all_articles = kpmg_articles + pwc_articles\n",
    "    themed_articles = []\n",
    "\n",
    "    print(f\"Processing {len(all_articles)} articles for theme and keyword extraction...\")\n",
    "\n",
    "    for idx, article in enumerate(all_articles, 1):\n",
    "        print(f\"[{idx}/{len(all_articles)}] Processing article: {article.get('title', 'No Title')}\")\n",
    "        themed_article = extract_themes_keywords(article)\n",
    "        themed_articles.append(themed_article)\n",
    "\n",
    "    # Save output JSON\n",
    "    with open(\"themed_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(themed_articles, f, indent=2)\n",
    "\n",
    "    print(\"Theme and keyword extraction completed. Results saved to themed_articles.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setx GEMINI_API_KEY \"AIzaSyDDgxbSk3pLA-BjvN5Xv0IIgImzWi6aXvE\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
