{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T12:44:44.702776Z",
     "iopub.status.busy": "2025-04-21T12:44:44.702591Z",
     "iopub.status.idle": "2025-04-21T12:44:46.519720Z",
     "shell.execute_reply": "2025-04-21T12:44:46.518950Z",
     "shell.execute_reply.started": "2025-04-21T12:44:44.702759Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/kpmgpwc/new.json\n",
      "/kaggle/input/kpmgpwc/classified_kpmg.json\n",
      "/kaggle/input/kpmgpwc/classified_pwc.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "/\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:45:10.962425Z",
     "iopub.status.busy": "2025-04-21T12:45:10.962015Z",
     "iopub.status.idle": "2025-04-21T12:45:14.731884Z",
     "shell.execute_reply": "2025-04-21T12:45:14.730780Z",
     "shell.execute_reply.started": "2025-04-21T12:45:10.962403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.27.0)\n",
      "Requirement already satisfied: pydantic<3.0.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: websockets<15.0dev,>=13.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0dev,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-genai) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T12:54:10.404439Z",
     "iopub.status.busy": "2025-04-21T12:54:10.403891Z",
     "iopub.status.idle": "2025-04-21T12:56:20.595700Z",
     "shell.execute_reply": "2025-04-21T12:56:20.595025Z",
     "shell.execute_reply.started": "2025-04-21T12:54:10.404414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API Key loaded successfully.\n",
      "DEBUG: Starting data loading...\n",
      "Successfully loaded data from /kaggle/input/kpmgpwc/classified_pwc.json\n",
      "Successfully loaded data from /kaggle/input/kpmgpwc/classified_kpmg.json\n",
      "DEBUG: Extended with 5 PWC articles.\n",
      "DEBUG: Extended with 34 KPMG articles.\n",
      "DEBUG: Loaded a total of 39 articles for analysis.\n",
      "\n",
      "Processing article 1/39: 'Sustainable packaging in the FMCG and retail sector'\n",
      "    Attempt 1/4 for article 1/39...\n",
      "    DEBUG: Successfully parsed JSON for article 1 on attempt 1.\n",
      "-> Analysis successful for article 1.\n",
      "DEBUG: Appended article 1. Current results count: 1\n",
      "\n",
      "Processing article 2/39: 'Electronics Component Manufacturing Scheme'\n",
      "    Attempt 1/4 for article 2/39...\n",
      "    DEBUG: Successfully parsed JSON for article 2 on attempt 1.\n",
      "-> Analysis successful for article 2.\n",
      "DEBUG: Appended article 2. Current results count: 2\n",
      "\n",
      "Processing article 3/39: 'Agentic AI in the human capital management (HCM) industry'\n",
      "    Attempt 1/4 for article 3/39...\n",
      "    DEBUG: Successfully parsed JSON for article 3 on attempt 1.\n",
      "-> Analysis successful for article 3.\n",
      "DEBUG: Appended article 3. Current results count: 3\n",
      "\n",
      "Processing article 4/39: 'Making the case for global workforce migration'\n",
      "    Attempt 1/4 for article 4/39...\n",
      "    DEBUG: Successfully parsed JSON for article 4 on attempt 1.\n",
      "-> Analysis successful for article 4.\n",
      "DEBUG: Appended article 4. Current results count: 4\n",
      "\n",
      "Processing article 5/39: 'Indian family offices – the new investors for India’s startup ecosystem'\n",
      "    Attempt 1/4 for article 5/39...\n",
      "    DEBUG: Successfully parsed JSON for article 5 on attempt 1.\n",
      "-> Analysis successful for article 5.\n",
      "DEBUG: Appended article 5. Current results count: 5\n",
      "\n",
      "Processing article 6/39: 'Top geopolitical risks 2025'\n",
      "    Attempt 1/4 for article 6/39...\n",
      "    DEBUG: Successfully parsed JSON for article 6 on attempt 1.\n",
      "-> Analysis successful for article 6.\n",
      "DEBUG: Appended article 6. Current results count: 6\n",
      "\n",
      "Processing article 7/39: '2025 Emerging Trends in Infrastructure and Transport: The Great Reset'\n",
      "    Attempt 1/4 for article 7/39...\n",
      "    DEBUG: Successfully parsed JSON for article 7 on attempt 1.\n",
      "-> Analysis successful for article 7.\n",
      "DEBUG: Appended article 7. Current results count: 7\n",
      "\n",
      "Processing article 8/39: 'Third-party risks in the pharmaceutical supply chain'\n",
      "    Attempt 1/4 for article 8/39...\n",
      "    DEBUG: Successfully parsed JSON for article 8 on attempt 1.\n",
      "-> Analysis successful for article 8.\n",
      "DEBUG: Appended article 8. Current results count: 8\n",
      "\n",
      "Processing article 9/39: 'Cybersecurity considerations 2025'\n",
      "    Attempt 1/4 for article 9/39...\n",
      "    DEBUG: Successfully parsed JSON for article 9 on attempt 1.\n",
      "-> Analysis successful for article 9.\n",
      "DEBUG: Appended article 9. Current results count: 9\n",
      "\n",
      "Processing article 10/39: 'Navigating Reserve Bank of India’s guidelines for NEFT and RTGS payment systems'\n",
      "    Attempt 1/4 for article 10/39...\n",
      "    DEBUG: Successfully parsed JSON for article 10 on attempt 1.\n",
      "-> Analysis successful for article 10.\n",
      "DEBUG: Appended article 10. Current results count: 10\n",
      "\n",
      "Processing article 11/39: 'Financial Crime Bulletin'\n",
      "    Attempt 1/4 for article 11/39...\n",
      "    DEBUG: Successfully parsed JSON for article 11 on attempt 1.\n",
      "-> Analysis successful for article 11.\n",
      "DEBUG: Appended article 11. Current results count: 11\n",
      "\n",
      "Processing article 12/39: 'KPMG global tech report: Technology insights'\n",
      "    Attempt 1/4 for article 12/39...\n",
      "    DEBUG: Successfully parsed JSON for article 12 on attempt 1.\n",
      "-> Analysis successful for article 12.\n",
      "DEBUG: Appended article 12. Current results count: 12\n",
      "\n",
      "Processing article 13/39: 'A new age of cybersecurity culture'\n",
      "    Attempt 1/4 for article 13/39...\n",
      "    DEBUG: Successfully parsed JSON for article 13 on attempt 1.\n",
      "-> Analysis successful for article 13.\n",
      "DEBUG: Appended article 13. Current results count: 13\n",
      "\n",
      "Processing article 14/39: 'Draft DPDP Rules: Guidance to DPDP Act implementation'\n",
      "    Attempt 1/4 for article 14/39...\n",
      "    DEBUG: Successfully parsed JSON for article 14 on attempt 1.\n",
      "-> Analysis successful for article 14.\n",
      "DEBUG: Appended article 14. Current results count: 14\n",
      "\n",
      "Processing article 15/39: 'Management Capability Development Index (MCDI) India 2024 Report'\n",
      "    Attempt 1/4 for article 15/39...\n",
      "    DEBUG: Successfully parsed JSON for article 15 on attempt 1.\n",
      "-> Analysis successful for article 15.\n",
      "DEBUG: Appended article 15. Current results count: 15\n",
      "\n",
      "Processing article 16/39: 'India’s digital dividend: The strategic roadmap towards becoming a global digital leader'\n",
      "    Attempt 1/4 for article 16/39...\n",
      "    DEBUG: Successfully parsed JSON for article 16 on attempt 1.\n",
      "-> Analysis successful for article 16.\n",
      "DEBUG: Appended article 16. Current results count: 16\n",
      "\n",
      "Processing article 17/39: 'KPMG 2024 CEO Outlook'\n",
      "    Attempt 1/4 for article 17/39...\n",
      "    DEBUG: Successfully parsed JSON for article 17 on attempt 1.\n",
      "-> Analysis successful for article 17.\n",
      "DEBUG: Appended article 17. Current results count: 17\n",
      "\n",
      "Processing article 18/39: 'You can with AI'\n",
      "    Attempt 1/4 for article 18/39...\n",
      "    DEBUG: Successfully parsed JSON for article 18 on attempt 1.\n",
      "-> Analysis successful for article 18.\n",
      "DEBUG: Appended article 18. Current results count: 18\n",
      "\n",
      "Processing article 19/39: 'Intelligent retail'\n",
      "    Attempt 1/4 for article 19/39...\n",
      "    DEBUG: Successfully parsed JSON for article 19 on attempt 1.\n",
      "-> Analysis successful for article 19.\n",
      "DEBUG: Appended article 19. Current results count: 19\n",
      "\n",
      "Processing article 20/39: 'Revolutionising traffic management and road safety with AI-powered intelligent traffic management system (ITMS)'\n",
      "    Attempt 1/4 for article 20/39...\n",
      "    DEBUG: Successfully parsed JSON for article 20 on attempt 1.\n",
      "-> Analysis successful for article 20.\n",
      "DEBUG: Appended article 20. Current results count: 20\n",
      "\n",
      "Processing article 21/39: 'KPMG global tech report: energy insights'\n",
      "    Attempt 1/4 for article 21/39...\n",
      "    DEBUG: Successfully parsed JSON for article 21 on attempt 1.\n",
      "-> Analysis successful for article 21.\n",
      "DEBUG: Appended article 21. Current results count: 21\n",
      "\n",
      "Processing article 22/39: 'KPMG global tech report – industrial manufacturing insights'\n",
      "    Attempt 1/4 for article 22/39...\n",
      "    DEBUG: Successfully parsed JSON for article 22 on attempt 1.\n",
      "-> Analysis successful for article 22.\n",
      "DEBUG: Appended article 22. Current results count: 22\n",
      "\n",
      "Processing article 23/39: 'KPMG global tech report 2024'\n",
      "    Attempt 1/4 for article 23/39...\n",
      "    DEBUG: Successfully parsed JSON for article 23 on attempt 1.\n",
      "-> Analysis successful for article 23.\n",
      "DEBUG: Appended article 23. Current results count: 23\n",
      "\n",
      "Processing article 24/39: 'Enhancing efficiency and effectiveness in policing with the crime and criminal tracking network and systems (CCTNS)'\n",
      "    Attempt 1/4 for article 24/39...\n",
      "    DEBUG: Successfully parsed JSON for article 24 on attempt 1.\n",
      "-> Analysis successful for article 24.\n",
      "DEBUG: Appended article 24. Current results count: 24\n",
      "\n",
      "Processing article 25/39: 'Establishing and operationalising the revenue augmentation and analytics cell'\n",
      "    Attempt 1/4 for article 25/39...\n",
      "    DEBUG: Successfully parsed JSON for article 25 on attempt 1.\n",
      "-> Analysis successful for article 25.\n",
      "DEBUG: Appended article 25. Current results count: 25\n",
      "\n",
      "Processing article 26/39: 'Evolving third party landscape in India's media and entertainment industry'\n",
      "    Attempt 1/4 for article 26/39...\n",
      "    DEBUG: Successfully parsed JSON for article 26 on attempt 1.\n",
      "-> Analysis successful for article 26.\n",
      "DEBUG: Appended article 26. Current results count: 26\n",
      "\n",
      "Processing article 27/39: 'Expected Credit Loss (ECL)'\n",
      "    Attempt 1/4 for article 27/39...\n",
      "    DEBUG: Successfully parsed JSON for article 27 on attempt 1.\n",
      "-> Analysis successful for article 27.\n",
      "DEBUG: Appended article 27. Current results count: 27\n",
      "\n",
      "Processing article 28/39: 'Forensic lens on related parties and transactions'\n",
      "    Attempt 1/4 for article 28/39...\n",
      "    DEBUG: Successfully parsed JSON for article 28 on attempt 1.\n",
      "-> Analysis successful for article 28.\n",
      "DEBUG: Appended article 28. Current results count: 28\n",
      "\n",
      "Processing article 29/39: 'Internal Risk Assessment guidance for money laundering/terrorist financing risks'\n",
      "    Attempt 1/4 for article 29/39...\n",
      "    DEBUG: Successfully parsed JSON for article 29 on attempt 1.\n",
      "-> Analysis successful for article 29.\n",
      "DEBUG: Appended article 29. Current results count: 29\n",
      "\n",
      "Processing article 30/39: 'India CX Report'25: Financial Services'\n",
      "    Attempt 1/4 for article 30/39...\n",
      "    DEBUG: Successfully parsed JSON for article 30 on attempt 1.\n",
      "-> Analysis successful for article 30.\n",
      "DEBUG: Appended article 30. Current results count: 30\n",
      "\n",
      "Processing article 31/39: 'India CX Report'25: Hotels'\n",
      "    Attempt 1/4 for article 31/39...\n",
      "    DEBUG: API call error for article 31 on attempt 1: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "    -> Rate limit exceeded.\n",
      "    -> API suggests waiting 38s.\n",
      "    -> Waiting for 41s before retry 2...\n",
      "    Attempt 2/4 for article 31/39...\n",
      "    DEBUG: Successfully parsed JSON for article 31 on attempt 2.\n",
      "-> Analysis successful for article 31.\n",
      "DEBUG: Appended article 31. Current results count: 31\n",
      "\n",
      "Processing article 32/39: 'India CX Report'25: Consumer Durables'\n",
      "    Attempt 1/4 for article 32/39...\n",
      "    DEBUG: Successfully parsed JSON for article 32 on attempt 1.\n",
      "-> Analysis successful for article 32.\n",
      "DEBUG: Appended article 32. Current results count: 32\n",
      "\n",
      "Processing article 33/39: 'India CX Report'25: Airlines'\n",
      "    Attempt 1/4 for article 33/39...\n",
      "    DEBUG: Successfully parsed JSON for article 33 on attempt 1.\n",
      "-> Analysis successful for article 33.\n",
      "DEBUG: Appended article 33. Current results count: 33\n",
      "\n",
      "Processing article 34/39: 'India CX Report'25: Real Estate'\n",
      "    Attempt 1/4 for article 34/39...\n",
      "    DEBUG: Successfully parsed JSON for article 34 on attempt 1.\n",
      "-> Analysis successful for article 34.\n",
      "DEBUG: Appended article 34. Current results count: 34\n",
      "\n",
      "Processing article 35/39: 'KPMG global tech  report: Healthcare insights'\n",
      "    Attempt 1/4 for article 35/39...\n",
      "    DEBUG: Successfully parsed JSON for article 35 on attempt 1.\n",
      "-> Analysis successful for article 35.\n",
      "DEBUG: Appended article 35. Current results count: 35\n",
      "\n",
      "Processing article 36/39: 'The rise of silver generation: Transforming the senior living landscape'\n",
      "    Attempt 1/4 for article 36/39...\n",
      "    DEBUG: Successfully parsed JSON for article 36 on attempt 1.\n",
      "-> Analysis successful for article 36.\n",
      "DEBUG: Appended article 36. Current results count: 36\n",
      "\n",
      "Processing article 37/39: 'Aviation leasing and financing ecosystem at GIFT IFSC, India'\n",
      "    Attempt 1/4 for article 37/39...\n",
      "    DEBUG: Successfully parsed JSON for article 37 on attempt 1.\n",
      "-> Analysis successful for article 37.\n",
      "DEBUG: Appended article 37. Current results count: 37\n",
      "\n",
      "Processing article 38/39: 'Food and Nutritional Security in India'\n",
      "    Attempt 1/4 for article 38/39...\n",
      "    DEBUG: Successfully parsed JSON for article 38 on attempt 1.\n",
      "-> Analysis successful for article 38.\n",
      "DEBUG: Appended article 38. Current results count: 38\n",
      "\n",
      "Processing article 39/39: 'Enabling infrastructure changes through policies for growth of EVs'\n",
      "    Attempt 1/4 for article 39/39...\n",
      "    DEBUG: Successfully parsed JSON for article 39 on attempt 1.\n",
      "-> Analysis successful for article 39.\n",
      "DEBUG: Appended article 39. Current results count: 39\n",
      "\n",
      "DEBUG: Finished processing loop. Total items in results list: 39\n",
      "DEBUG: First item in results (showing keys): ['title', 'url', 'publish_date', 'pdf_path', 'cleaned_text', 'industry', 'analysis']\n",
      "DEBUG: Analysis field in first item: True\n",
      "\n",
      "Attempting to save analysis results to /kaggle/working/analysis_results_debug.json...\n",
      "Results saved successfully.\n",
      "DEBUG: Output file '/kaggle/working/analysis_results_debug.json' created with size: 1799212 bytes.\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "try:\n",
    "    from google.api_core.exceptions import ResourceExhausted\n",
    "except ImportError:\n",
    "    ResourceExhausted = None \n",
    "# --- Configuration (Keep previous config: API Key, Paths, Prompt, Model Config) ---\n",
    "\n",
    "# 1. Get API Key from Kaggle Secrets\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    gemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    print(\"Gemini API Key loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Gemini API Key from Kaggle Secrets: {e}\")\n",
    "    print(\"Please ensure you have added your key as a secret named 'GEMINI_API_KEY' and attached it to this notebook.\")\n",
    "    gemini_api_key = None\n",
    "\n",
    "# 2. Define Input File Paths\n",
    "input_dir = \"/kaggle/input/kpmgpwc\" \n",
    "pwc_file_path = os.path.join(input_dir, \"classified_pwc.json\")\n",
    "kpmg_file_path = os.path.join(input_dir, \"classified_kpmg.json\")\n",
    "output_file_path = \"/kaggle/working/analysis_results_debug.json\" \n",
    "\n",
    "\n",
    "# 3. Define the Prompt (Keep the same prompt as before)\n",
    "prompt = \"\"\"\n",
    "Analyze the following article text and extract the following information:\n",
    "\n",
    "1.  **The main theme of the article (1-2 sentences).** Capture the overarching message or argument.\n",
    "2.  **The primary topic (a short phrase or sentence).** Identify the core subject matter.\n",
    "3.  **5-10 significant keywords or key phrases that best represent the article’s content.** These should be terms that quickly convey the main concepts.\n",
    "4.  **For each keyword or phrase, provide a similarity score (a float between 0.0 and 1.0) indicating how closely it matches the article’s central content.** A score of 1.0 means a perfect match, while 0.0 means no relevance.\n",
    "\n",
    "Return the output *only* in the following JSON format. Do not include any text before or after the JSON structure:\n",
    "\n",
    "json\n",
    "{\n",
    "  \"theme\": \"string\",\n",
    "  \"topic\": \"string\",\n",
    "  \"keywords\": [\n",
    "    {\"keyword\": \"string\", \"similarity_score\": 0.0},\n",
    "    {\"keyword\": \"string\", \"similarity_score\": 0.0},\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "Here is the article text:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 4. Configure the Gemini Model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 2048,\n",
    "  \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "]\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\",\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)\n",
    "\n",
    "# --- Helper Functions  ---\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Loads JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Successfully loaded data from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"DEBUG: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"DEBUG: Could not decode JSON from {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: An unexpected error occurred loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- UPDATED Analysis Function with Rate Limit Handling & Debug Prints ---\n",
    "\n",
    "def analyze_article(article_text, article_index, total_articles):\n",
    "    \"\"\"\n",
    "    Sends article text to Gemini for analysis, handles rate limits with retries,\n",
    "    and returns parsed JSON. Includes debugging prints.\n",
    "    \"\"\"\n",
    "    if not article_text or not isinstance(article_text, str) or len(article_text.strip()) < 50: # Increased min length check slightly\n",
    "        print(f\"DEBUG: Skipping analysis for article {article_index+1} due to empty or short article text.\")\n",
    "        return None\n",
    "\n",
    "    full_prompt = prompt + article_text\n",
    "    max_retries = 4 \n",
    "    base_wait_time = 10 \n",
    "    attempt = 0\n",
    "    response = None # Initialize response variable\n",
    "\n",
    "    while attempt < max_retries:\n",
    "        attempt += 1\n",
    "        print(f\"    Attempt {attempt}/{max_retries} for article {article_index+1}/{total_articles}...\")\n",
    "        try:\n",
    "            response = model.generate_content(full_prompt)\n",
    "\n",
    "            # --- DEBUG: Print raw response text ---\n",
    "            # print(f\"    DEBUG: Raw response text (attempt {attempt}):\\n---\\n{response.text}\\n---\")\n",
    "            # --- END DEBUG ---\n",
    "\n",
    "            # Check for content blocking *before* trying to parse JSON\n",
    "            if not response.candidates:\n",
    "                 print(f\"    DEBUG: Response blocked (no candidates) for article {article_index+1} on attempt {attempt}. Likely safety/policy.\")\n",
    "                 if response.prompt_feedback:\n",
    "                     print(f\"    DEBUG: Prompt Feedback: {response.prompt_feedback}\")\n",
    "                 else:\n",
    "                     print(\"    DEBUG: No prompt feedback available for blocked response.\")\n",
    "                 return None # Don't retry if blocked\n",
    "\n",
    "            # Check if response text is empty or too short\n",
    "            if not response.text or len(response.text.strip()) < 10:\n",
    "                 print(f\"    DEBUG: Empty or very short response text received for article {article_index+1} on attempt {attempt}.\")\n",
    "                 return None # Treat as failure, don't retry\n",
    "\n",
    "            # Attempt to directly parse the response text as JSON\n",
    "            analysis_json = json.loads(response.text)\n",
    "            print(f\"    DEBUG: Successfully parsed JSON for article {article_index+1} on attempt {attempt}.\")\n",
    "            return analysis_json # Success! Exit the loop and return\n",
    "\n",
    "        except json.JSONDecodeError as json_e:\n",
    "            print(f\"    DEBUG: Failed to parse JSON for article {article_index+1} on attempt {attempt}. Error: {json_e}\")\n",
    "            print(\"    ----- Gemini Response Text -----\")\n",
    "            try:\n",
    "                 print(response.text if response else \"No response object available\")\n",
    "            except Exception as e_print:\n",
    "                print(f\"    (Could not print response text: {e_print})\")\n",
    "            print(\"    -------------------------------\")\n",
    "            # Don't automatically retry on JSON errors\n",
    "            return None # Failed to parse\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            print(f\"    DEBUG: API call error for article {article_index+1} on attempt {attempt}: {error_message}\")\n",
    "\n",
    "            is_rate_limit_error = (\"429\" in error_message and \"exceeded your current quota\" in error_message.lower())\n",
    "            is_google_resource_exhausted = ResourceExhausted and isinstance(e, ResourceExhausted)\n",
    "\n",
    "            if is_rate_limit_error or is_google_resource_exhausted:\n",
    "                print(\"    -> Rate limit exceeded.\")\n",
    "                # Try to extract retry_delay using regex\n",
    "                match = re.search(r\"retry_delay {\\s*seconds: (\\d+)\\s*}\", error_message)\n",
    "                wait_time = base_wait_time # Default if no match\n",
    "\n",
    "                if match:\n",
    "                    delay_seconds = int(match.group(1))\n",
    "                    wait_time = delay_seconds + 3 # Add a 3-second buffer\n",
    "                    print(f\"    -> API suggests waiting {delay_seconds}s.\")\n",
    "                else:\n",
    "                    # Use exponential backoff if no specific delay found\n",
    "                    wait_time = base_wait_time * (2 ** (attempt - 1)) # Exponential backoff\n",
    "                    print(f\"    -> No specific retry_delay found. Using backoff.\")\n",
    "\n",
    "                if attempt >= max_retries:\n",
    "                     print(f\"    -> Max retries ({max_retries}) reached after rate limit. Giving up on article {article_index+1}.\")\n",
    "                     return None # Exit after max retries\n",
    "\n",
    "                print(f\"    -> Waiting for {wait_time}s before retry {attempt+1}...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue # Go to the next iteration of the while loop for retry\n",
    "\n",
    "            else:\n",
    "                # Handle other errors\n",
    "                print(f\"    -> Non-rate-limit error encountered. Not retrying article {article_index+1}.\")\n",
    "                # Check for prompt feedback in case of other blocking issues\n",
    "                try:\n",
    "                     if response and response.prompt_feedback:\n",
    "                        print(f\"    DEBUG: Prompt Feedback on other error: {response.prompt_feedback}\")\n",
    "                except AttributeError:\n",
    "                     print(\"   DEBUG: No prompt feedback attribute on response object.\")\n",
    "                except Exception as feedback_e:\n",
    "                     print(f\"   DEBUG: Error accessing prompt feedback: {feedback_e}\")\n",
    "                return None # Exit loop for other errors\n",
    "\n",
    "    # If loop finishes without returning (e.g., max retries reached)\n",
    "    print(f\"    DEBUG: Reached end of analyze_article for article {article_index+1} after {attempt} attempts without success.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if gemini_api_key: \n",
    "    print(\"DEBUG: Starting data loading...\")\n",
    "    pwc_data = load_json_data(pwc_file_path)\n",
    "    kpmg_data = load_json_data(kpmg_file_path)\n",
    "\n",
    "    all_articles = []\n",
    "    if pwc_data and isinstance(pwc_data, list): \n",
    "        all_articles.extend(pwc_data)\n",
    "        print(f\"DEBUG: Extended with {len(pwc_data)} PWC articles.\")\n",
    "    else:\n",
    "        print(f\"DEBUG: PWC data not loaded or not a list.\")\n",
    "\n",
    "    if kpmg_data and isinstance(kpmg_data, list): \n",
    "        all_articles.extend(kpmg_data)\n",
    "        print(f\"DEBUG: Extended with {len(kpmg_data)} KPMG articles.\")\n",
    "    else:\n",
    "        print(f\"DEBUG: KPMG data not loaded or not a list.\")\n",
    "\n",
    "\n",
    "    if not all_articles:\n",
    "        print(\"DEBUG: No article data loaded or combined list is empty. Exiting.\")\n",
    "    else:\n",
    "        print(f\"DEBUG: Loaded a total of {len(all_articles)} articles for analysis.\")\n",
    "        results = []\n",
    "        total_articles = len(all_articles)\n",
    "\n",
    "        for i, article in enumerate(all_articles):\n",
    "            # Defensive copy to avoid modifying the original list directly if needed elsewhere\n",
    "            article_copy = article.copy()\n",
    "            title = article_copy.get('title', f'Article {i+1} (No Title)')\n",
    "            print(f\"\\nProcessing article {i+1}/{total_articles}: '{title}'\")\n",
    "            text_to_analyze = article_copy.get('cleaned_text')\n",
    "\n",
    "            if text_to_analyze:\n",
    "                analysis_result = analyze_article(text_to_analyze, i, total_articles) \n",
    "                if analysis_result:\n",
    "                    article_copy['analysis'] = analysis_result \n",
    "                    print(f\"-> Analysis successful for article {i+1}.\")\n",
    "                else:\n",
    "                    article_copy['analysis'] = None \n",
    "                    print(f\"-> Analysis failed/skipped for article {i+1}.\")\n",
    "            else:\n",
    "                print(f\"-> Skipping analysis for article {i+1}: No 'cleaned_text' found.\")\n",
    "                article_copy['analysis'] = None\n",
    "\n",
    "            results.append(article_copy) # Append the processed copy\n",
    "            print(f\"DEBUG: Appended article {i+1}. Current results count: {len(results)}\")\n",
    "\n",
    "\n",
    "        # --- DEBUG: Check results list before saving ---\n",
    "        print(f\"\\nDEBUG: Finished processing loop. Total items in results list: {len(results)}\")\n",
    "        if results:\n",
    "             print(f\"DEBUG: First item in results (showing keys): {list(results[0].keys())}\")\n",
    "             if 'analysis' in results[0]:\n",
    "                  print(f\"DEBUG: Analysis field in first item: {results[0]['analysis'] is not None}\")\n",
    "        else:\n",
    "             print(\"DEBUG: Results list is empty before saving!\")\n",
    "        # --- END DEBUG ---\n",
    "\n",
    "        # Save results to a new JSON file\n",
    "        print(f\"\\nAttempting to save analysis results to {output_file_path}...\")\n",
    "        try:\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "            print(\"Results saved successfully.\")\n",
    "            # --- DEBUG: Verify file exists and has size ---\n",
    "            if os.path.exists(output_file_path):\n",
    "                file_size = os.path.getsize(output_file_path)\n",
    "                print(f\"DEBUG: Output file '{output_file_path}' created with size: {file_size} bytes.\")\n",
    "            else:\n",
    "                print(f\"DEBUG: Output file '{output_file_path}' was NOT created.\")\n",
    "            # --- END DEBUG ---\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG: Error saving results to file: {e}\")\n",
    "\n",
    "        print(\"\\nProcessing complete.\")\n",
    "else:\n",
    "    print(\"DEBUG: Gemini API Key not configured. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:12:17.101879Z",
     "iopub.status.busy": "2025-04-21T13:12:17.101600Z",
     "iopub.status.idle": "2025-04-21T13:12:39.684535Z",
     "shell.execute_reply": "2025-04-21T13:12:39.683469Z",
     "shell.execute_reply.started": "2025-04-21T13:12:17.101857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API Key loaded successfully.\n",
      "--- Starting Clustering Phase ---\n",
      "Successfully loaded data from /kaggle/working/analysis_results_debug.json\n",
      "Extracting valid themes for clustering...\n",
      "Extracted 39 valid themes.\n",
      "Generating embeddings for 39 texts using models/embedding-001...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f0397054314455b5fff4d1ea28f630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Embeddings:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete.\n",
      "Successfully generated 39 embeddings.\n",
      "Calculating Silhouette Scores for k=2 to 15...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9081e57bf3e40838ce300b99debe23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding Optimal K:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  k=2, Silhouette Score: 0.0671\n",
      "  k=3, Silhouette Score: 0.0649\n",
      "  k=4, Silhouette Score: 0.0691\n",
      "  k=5, Silhouette Score: 0.0399\n",
      "  k=6, Silhouette Score: 0.0616\n",
      "  k=7, Silhouette Score: 0.0439\n",
      "  k=8, Silhouette Score: 0.0692\n",
      "  k=9, Silhouette Score: 0.0504\n",
      "  k=10, Silhouette Score: 0.0542\n",
      "  k=11, Silhouette Score: 0.0504\n",
      "  k=12, Silhouette Score: 0.0589\n",
      "  k=13, Silhouette Score: 0.0569\n",
      "  k=14, Silhouette Score: 0.0571\n",
      "  k=15, Silhouette Score: 0.0589\n",
      "\n",
      "Optimal k based on Silhouette Score: 8\n",
      "\n",
      "Performing KMeans clustering with k=8...\n",
      "Clustering complete.\n",
      "Organizing results by cluster...\n",
      "Results organized.\n",
      "\n",
      "Saving clustered results to /kaggle/working/clustered_analysis_results.json...\n",
      "Clustered results saved successfully.\n",
      "DEBUG: Output file '/kaggle/working/clustered_analysis_results.json' created with size: 19415 bytes.\n",
      "\n",
      "Clustering script finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "try:\n",
    "    from google.api_core.exceptions import ResourceExhausted\n",
    "except ImportError:\n",
    "    ResourceExhausted = None\n",
    "\n",
    "# --- Add clustering imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Get API Key\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    gemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    print(\"Gemini API Key loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Gemini API Key from Kaggle Secrets: {e}\")\n",
    "    gemini_api_key = None\n",
    "\n",
    "# 2. Define Paths \n",
    "input_analysis_file = \"/kaggle/working/analysis_results_debug.json\" \n",
    "output_clustered_file = \"/kaggle/working/clustered_analysis_results.json\" \n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"Loads JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Successfully loaded data from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_embeddings(texts, model_name=\"models/embedding-001\"):\n",
    "    \"\"\"Generates embeddings for a list of texts using Gemini.\"\"\"\n",
    "    print(f\"Generating embeddings for {len(texts)} texts using {model_name}...\")\n",
    "    embeddings = []\n",
    "    # Use tqdm for progress indication\n",
    "    for text in tqdm(texts, desc=\"Generating Embeddings\"):\n",
    "        try:\n",
    "            result = genai.embed_content(model=model_name, content=text, task_type=\"SEMANTIC_SIMILARITY\") # or \"CLUSTERING\"\n",
    "            embeddings.append(result['embedding'])\n",
    "            time.sleep(0.1) # Small delay to help manage potential rate limits on embedding endpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not generate embedding for text chunk: '{text[:50]}...'. Error: {e}. Appending None.\")\n",
    "            embeddings.append(None) \n",
    "    print(\"Embedding generation complete.\")\n",
    "    return embeddings\n",
    "\n",
    "def find_optimal_k_silhouette(embeddings_array, max_k=15, min_k=2):\n",
    "    \"\"\"Finds the optimal number of clusters using the Silhouette Score.\"\"\"\n",
    "    if embeddings_array.shape[0] < min_k:\n",
    "         print(f\"Not enough samples ({embeddings_array.shape[0]}) for clustering (min_k={min_k}). Returning k=1.\")\n",
    "         return 1\n",
    "    if embeddings_array.shape[0] <= max_k: \n",
    "        max_k = embeddings_array.shape[0] - 1\n",
    "        print(f\"Adjusted max_k to {max_k} due to low number of samples.\")\n",
    "    if min_k > max_k:\n",
    "        print(f\"min_k ({min_k}) cannot be greater than adjusted max_k ({max_k}). Returning k=1.\")\n",
    "        return 1\n",
    "    if min_k < 2:\n",
    "        print(\"min_k must be at least 2 for silhouette score. Returning k=1.\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "    print(f\"Calculating Silhouette Scores for k={min_k} to {max_k}...\")\n",
    "    silhouette_scores = []\n",
    "    k_range = range(min_k, max_k + 1)\n",
    "\n",
    "    for k in tqdm(k_range, desc=\"Finding Optimal K\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) \n",
    "        cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "        # Check if we have more than one cluster and enough samples to calculate silhouette score\n",
    "            if len(set(cluster_labels)) > 1 and len(cluster_labels) > k:\n",
    "             score = silhouette_score(embeddings_array, cluster_labels)\n",
    "             silhouette_scores.append(score)\n",
    "             print(f\"  k={k}, Silhouette Score: {score:.4f}\")\n",
    "        else:\n",
    "             print(f\"  k={k}, Cannot calculate Silhouette Score (likely not enough distinct clusters formed). Appending -1.\")\n",
    "             silhouette_scores.append(-1)\n",
    "\n",
    "    if not silhouette_scores or max(silhouette_scores) <= -1:\n",
    "        print(\"Warning: Could not determine optimal k using Silhouette Score. Defaulting to k=3.\")\n",
    "        return 3 \n",
    "\n",
    "    optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"\\nOptimal k based on Silhouette Score: {optimal_k}\")\n",
    "    return optimal_k\n",
    "\n",
    "# --- Main Execution: Clustering Phase ---\n",
    "\n",
    "if gemini_api_key: \n",
    "    print(\"--- Starting Clustering Phase ---\")\n",
    "    # 1. Load the previously analyzed data\n",
    "    analyzed_articles = load_json_data(input_analysis_file)\n",
    "\n",
    "    if not analyzed_articles:\n",
    "        print(\"Could not load analyzed articles. Exiting clustering phase.\")\n",
    "    else:\n",
    "        # 2. Prepare data: Extract themes and keep track of original articles\n",
    "        themes_to_cluster = []\n",
    "        article_references = [] # Store index or title/url to link back\n",
    "        valid_indices = [] # Keep track of the original index of articles with valid themes\n",
    "\n",
    "        print(\"Extracting valid themes for clustering...\")\n",
    "        for i, article in enumerate(analyzed_articles):\n",
    "            analysis = article.get('analysis')\n",
    "            if analysis and isinstance(analysis, dict) and analysis.get('theme'):\n",
    "                theme_text = analysis['theme'].strip()\n",
    "                if len(theme_text) > 10: # Basic check for non-empty theme\n",
    "                    themes_to_cluster.append(theme_text)\n",
    "                    # Store the original index and title/url for reference\n",
    "                    article_references.append({\n",
    "                        \"original_index\": i,\n",
    "                        \"title\": article.get('title', f\"Article_{i}\"),\n",
    "                        \"url\": article.get('url', 'N/A')\n",
    "                    })\n",
    "                    valid_indices.append(i)\n",
    "                else:\n",
    "                    print(f\"  Skipping article {i+1} (Title: {article.get('title', 'N/A')}) due to short/empty theme.\")\n",
    "            else:\n",
    "                 print(f\"  Skipping article {i+1} (Title: {article.get('title', 'N/A')}) due to missing or invalid analysis/theme.\")\n",
    "\n",
    "        if not themes_to_cluster:\n",
    "            print(\"No valid themes found to cluster. Exiting.\")\n",
    "        else:\n",
    "            print(f\"Extracted {len(themes_to_cluster)} valid themes.\")\n",
    "\n",
    "            # 3. Generate Embeddings for the themes\n",
    "            embeddings = generate_embeddings(themes_to_cluster)\n",
    "\n",
    "            # Filter out any None embeddings that might have resulted from API errors\n",
    "            valid_embeddings = [emb for emb in embeddings if emb is not None]\n",
    "            # Adjust references and themes to match the valid embeddings\n",
    "            valid_themes = [themes_to_cluster[i] for i, emb in enumerate(embeddings) if emb is not None]\n",
    "            valid_references = [article_references[i] for i, emb in enumerate(embeddings) if emb is not None]\n",
    "\n",
    "            if not valid_embeddings:\n",
    "                print(\"Error: Embedding generation failed for all themes. Cannot proceed with clustering.\")\n",
    "            else:\n",
    "                print(f\"Successfully generated {len(valid_embeddings)} embeddings.\")\n",
    "                embeddings_array = np.array(valid_embeddings) # Convert to NumPy array for scikit-learn\n",
    "\n",
    "                # 4. Find the optimal number of clusters (k)\n",
    "                num_samples = embeddings_array.shape[0]\n",
    "                potential_max_k = min(15, num_samples -1 if num_samples > 1 else 1) # Cannot have more clusters than samples - 1\n",
    "                optimal_k = find_optimal_k_silhouette(embeddings_array, max_k=potential_max_k)\n",
    "\n",
    "                if optimal_k < 2:\n",
    "                    print(\"Optimal k is less than 2, clustering is not meaningful. Saving unclustered results.\")\n",
    "                    clustered_results = {\"unclustered_themes\": []}\n",
    "                    for i in range(len(valid_themes)):\n",
    "                        clustered_results[\"unclustered_themes\"].append({\n",
    "                            \"title\": valid_references[i][\"title\"],\n",
    "                            \"url\": valid_references[i][\"url\"],\n",
    "                            \"theme\": valid_themes[i],\n",
    "                        })\n",
    "                else:\n",
    "                    # 5. Perform KMeans clustering\n",
    "                    print(f\"\\nPerforming KMeans clustering with k={optimal_k}...\")\n",
    "                    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "                    cluster_labels = kmeans.fit_predict(embeddings_array)\n",
    "                    print(\"Clustering complete.\")\n",
    "\n",
    "                    # 6. Organize results by cluster\n",
    "                    print(\"Organizing results by cluster...\")\n",
    "                    clustered_results = {}\n",
    "                    for i in range(optimal_k):\n",
    "                        clustered_results[f\"cluster_{i}\"] = []\n",
    "\n",
    "                    for i, label in enumerate(cluster_labels):\n",
    "                        cluster_name = f\"cluster_{label}\"\n",
    "                        clustered_results[cluster_name].append({\n",
    "                            \"title\": valid_references[i][\"title\"],\n",
    "                            \"url\": valid_references[i][\"url\"],\n",
    "                            \"theme\": valid_themes[i],\n",
    "                            \n",
    "                        })\n",
    "                    print(\"Results organized.\")\n",
    "\n",
    "\n",
    "                # 7. Save clustered results\n",
    "                print(f\"\\nSaving clustered results to {output_clustered_file}...\")\n",
    "                try:\n",
    "                    with open(output_clustered_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(clustered_results, f, indent=4, ensure_ascii=False)\n",
    "                    print(\"Clustered results saved successfully.\")\n",
    "                    # --- DEBUG: Verify file exists and has size ---\n",
    "                    if os.path.exists(output_clustered_file):\n",
    "                        file_size = os.path.getsize(output_clustered_file)\n",
    "                        print(f\"DEBUG: Output file '{output_clustered_file}' created with size: {file_size} bytes.\")\n",
    "                    else:\n",
    "                        print(f\"DEBUG: Output file '{output_clustered_file}' was NOT created.\")\n",
    "                    # --- END DEBUG ---\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving clustered results to file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Gemini API Key not configured. Cannot proceed with embedding/clustering.\")\n",
    "\n",
    "print(\"\\nClustering script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7201358,
     "sourceId": 11495656,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
